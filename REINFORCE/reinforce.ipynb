{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-02T11:31:45.287791Z","iopub.status.busy":"2022-03-02T11:31:45.287429Z","iopub.status.idle":"2022-03-02T11:31:45.293931Z","shell.execute_reply":"2022-03-02T11:31:45.292394Z","shell.execute_reply.started":"2022-03-02T11:31:45.287759Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","np.random.seed(1)\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import precision_score\n","import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:05:07.163245Z","iopub.status.busy":"2022-03-02T12:05:07.162844Z","iopub.status.idle":"2022-03-02T12:05:13.453919Z","shell.execute_reply":"2022-03-02T12:05:13.452446Z","shell.execute_reply.started":"2022-03-02T12:05:07.163206Z"},"trusted":true},"outputs":[],"source":["def load_df(path):\n","    df = pd.read_parquet(path)\n","    features = [column for column in df.columns if 'feature' in column]\n","    return df, features\n","\n","\n","def preprocess(df_from_parquet, features):\n","    # delete all trading oportunities that are not taken into account for \n","    # utility score calculation\n","    df_from_parquet = df_from_parquet.query('weight > 0').reset_index(drop = True)\n","    return df_from_parquet\n","\n","data_df, features = load_df('/kaggle/input/janestreet-parquet/train.parquet')\n","data_df = preprocess(data_df, features)\n","\n","n_samples = data_df.shape[0]\n","train_df = data_df[:int(0.9 * n_samples)]\n","test_df = data_df[int(0.9 * n_samples):int(0.95 * n_samples)]\n","valid_df = data_df[int(0.95 * n_samples):]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:05:25.846355Z","iopub.status.busy":"2022-03-02T12:05:25.845713Z","iopub.status.idle":"2022-03-02T12:05:25.850716Z","shell.execute_reply":"2022-03-02T12:05:25.849706Z","shell.execute_reply.started":"2022-03-02T12:05:25.846313Z"},"trusted":true},"outputs":[],"source":["imp = SimpleImputer(missing_values = np.nan , strategy = 'constant', fill_value = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:05:27.27052Z","iopub.status.busy":"2022-03-02T12:05:27.270133Z","iopub.status.idle":"2022-03-02T12:05:27.299685Z","shell.execute_reply":"2022-03-02T12:05:27.298655Z","shell.execute_reply.started":"2022-03-02T12:05:27.270486Z"},"trusted":true},"outputs":[],"source":["# base agents\n","\n","class Agent(object):\n","    # Our policy that maps state to action parameterized by w\n","    def policy(self, state):     \n","        raise NotImplementedError('You need to overwrite the policy method.')\n","        \n","    def predict(self, *state):\n","        return self.policy(state)\n","    \n","    def train(self, *state):\n","        return self.policy(state)\n","    \n","    def store_reward(self, reward):\n","        pass\n","\n","    def update(self):\n","        pass\n","\n","    # Vectorized softmax Jacobian\n","    @staticmethod\n","    def softmax_grad(softmax):\n","        s = softmax.reshape(-1,1)\n","        return np.diagflat(s) - np.dot(s, s.T)\n","\n","\n","class RandomAgent(Agent):\n","    def __init__(self, n_actions):\n","        self.n_actions = n_actions\n","    \n","    def policy(self, state):\n","        return np.random.binomial(1, 1 / self.n_actions)\n","        \n","        \n","class AlwaysTradeAgent(Agent):\n","    def policy(self, state):\n","        return 1\n","    \n","    \n","class REINFORCE(Agent):\n","    def __init__(self, state_dim, n_actions, learning_rate, gamma, train):\n","        # Init weight\n","        self.w = np.random.rand(state_dim, n_actions) * 0.1\n","        self.n_actions = n_actions\n","        self.lr = learning_rate\n","        self.g = gamma\n","        self.grads = []\n","        self.rewards = []\n","        self._train = train\n","                   \n","    @staticmethod\n","    def preprocess_state(state):\n","        return imp.fit_transform(np.array([state]).reshape((1, -1)))\n","        \n","    # Our policy that maps state to action parameterized by w\n","    def policy(self, state):\n","        exp = np.exp(state.dot(self.w))\n","        probs = exp / np.sum(exp)\n","        action = np.random.choice(self.n_actions, p=probs[0])\n","        return action, probs\n","\n","    def train(self, state):\n","        state = self.preprocess_state(state)\n","        action, probs = self.policy(state)\n","        dsoftmax = self.softmax_grad(probs)[action,:]\n","        dlog = dsoftmax / probs[0, action]\n","        grad = state.T.dot(dlog[None,:])\n","        self.grads.append(grad)\n","        return action\n","    \n","    def predict(self, state):\n","        if self._train:\n","            return self.train(state)\n","        else:\n","            state = self.preprocess_state(state)\n","            return np.argmax(self.policy(state)[1][0])\n","        \n","    def store_reward(self, reward):\n","        # Compute gradient and save with reward in memory for our weight update\n","        self.rewards.append(reward)\n","\n","    def update(self):\n","        for i in range(len(self.grads)):\n","            # Loop through everything that happend in the episode and update towards the log policy gradient times **FUTURE** reward\n","            self.w += self.lr * self.grads[i] * sum([r * (self.g ** r) for t, r in enumerate(self.rewards[i:])])\n","        self.grads = []\n","        self.rewards = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:05:28.420201Z","iopub.status.busy":"2022-03-02T12:05:28.419438Z","iopub.status.idle":"2022-03-02T12:05:28.438931Z","shell.execute_reply":"2022-03-02T12:05:28.437806Z","shell.execute_reply.started":"2022-03-02T12:05:28.42014Z"},"trusted":true},"outputs":[],"source":["# simulation environment\n","\n","class SimulationEnv(object):\n","    def __init__(self, df):\n","        self.predictions = []\n","        self.rewards = []\n","        self.utility = 0\n","        self.ps = []\n","\n","        self.df = df\n","        \n","\n","    def p(self, step_df, agent):\n","        result = 0\n","        X = step_df.loc[:, step_df.columns.str.contains('feature')].values\n","        Y = step_df.eval('weight * resp')\n","        for i in range(X.shape[0]):\n","            pred = agent.predict(X[i])\n","            self.predictions.append(pred)\n","            reward = pred * Y.iloc[i]\n","            self.rewards.append(reward)\n","            agent.store_reward(reward)\n","            result += reward\n","            \n","            agent.update()\n","\n","        return result\n","\n","    def simulate(self, agent):\n","        ps = []\n","        for i in tqdm.tqdm(range(self.df.date.min(), self.df.date.max() + 1)):\n","            ps.append(self.p(self.df[self.df.date == i], agent))\n","\n","        t = np.multiply(np.sum (ps) / np.sqrt(np.sum(np.power(ps, 2))), np.sqrt(250/len(ps)))\n","\n","        utility = np.multiply(np.min([np.max([t, 0]), 6]), np.sum(ps))\n","        self.ps = ps\n","        \n","        self.utility = utility\n","        return utility, ps\n","    \n","    def reset(self):\n","        self.predictions = []\n","        self.rewards = []\n","        self.utility = 0\n","        \n","    def print_results(self):\n","\n","        pred = self.predictions\n","        y = (self.df['resp'] > 0) * 1\n","        \n","        print(f'utility {self.utility}')\n","        print(f'precision {precision_score(y, pred)}')\n","        plt.plot(self.ps);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:05:28.619336Z","iopub.status.busy":"2022-03-02T12:05:28.618937Z","iopub.status.idle":"2022-03-02T12:05:28.690173Z","shell.execute_reply":"2022-03-02T12:05:28.688805Z","shell.execute_reply.started":"2022-03-02T12:05:28.619304Z"},"trusted":true},"outputs":[],"source":["env = SimulationEnv(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:05:29.865451Z","iopub.status.busy":"2022-03-02T12:05:29.865029Z","iopub.status.idle":"2022-03-02T12:06:15.53753Z","shell.execute_reply":"2022-03-02T12:06:15.536323Z","shell.execute_reply.started":"2022-03-02T12:05:29.865413Z"},"trusted":true},"outputs":[],"source":["env.reset()\n","rd_agent = RandomAgent(2)\n","env.simulate(rd_agent)\n","\n","env.print_results()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:06:15.540804Z","iopub.status.busy":"2022-03-02T12:06:15.540354Z","iopub.status.idle":"2022-03-02T12:06:49.262801Z","shell.execute_reply":"2022-03-02T12:06:49.261813Z","shell.execute_reply.started":"2022-03-02T12:06:15.540758Z"},"trusted":true},"outputs":[],"source":["env.reset()\n","at_agent = AlwaysTradeAgent()\n","env.simulate(at_agent)\n","\n","env.print_results()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:06:49.264821Z","iopub.status.busy":"2022-03-02T12:06:49.264448Z","iopub.status.idle":"2022-03-02T12:20:32.75408Z","shell.execute_reply":"2022-03-02T12:20:32.753286Z","shell.execute_reply.started":"2022-03-02T12:06:49.264788Z"},"trusted":true},"outputs":[],"source":["env.reset()\n","re_agent = REINFORCE(130, 2, 0.001, 0.999, True)\n","env.simulate(re_agent)\n","\n","env.print_results()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:20:32.756018Z","iopub.status.busy":"2022-03-02T12:20:32.755495Z","iopub.status.idle":"2022-03-02T12:20:33.007114Z","shell.execute_reply":"2022-03-02T12:20:33.006172Z","shell.execute_reply.started":"2022-03-02T12:20:32.755982Z"},"trusted":true},"outputs":[],"source":["env = SimulationEnv(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:20:33.010958Z","iopub.status.busy":"2022-03-02T12:20:33.010487Z","iopub.status.idle":"2022-03-02T12:20:35.572088Z","shell.execute_reply":"2022-03-02T12:20:35.571226Z","shell.execute_reply.started":"2022-03-02T12:20:33.010907Z"},"trusted":true},"outputs":[],"source":["env.reset()\n","env.simulate(rd_agent)\n","\n","env.print_results()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:20:35.574343Z","iopub.status.busy":"2022-03-02T12:20:35.57389Z","iopub.status.idle":"2022-03-02T12:20:37.558779Z","shell.execute_reply":"2022-03-02T12:20:37.557889Z","shell.execute_reply.started":"2022-03-02T12:20:35.574297Z"},"trusted":true},"outputs":[],"source":["env.reset()\n","env.simulate(at_agent)\n","\n","env.print_results()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T12:20:37.562558Z","iopub.status.busy":"2022-03-02T12:20:37.562239Z","iopub.status.idle":"2022-03-02T12:21:15.103059Z","shell.execute_reply":"2022-03-02T12:21:15.102196Z","shell.execute_reply.started":"2022-03-02T12:20:37.562528Z"},"trusted":true},"outputs":[],"source":["re_agent._train = False\n","\n","env.reset()\n","env.simulate(re_agent)\n","\n","env.print_results()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
